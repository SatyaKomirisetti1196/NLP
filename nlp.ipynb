{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3gxuwdtur0s"
      },
      "source": [
        "# NLP Processing Assignment\n",
        "## Part C - Coding Questions\n",
        "\n",
        "- **Student Name:** Satya Komirisetti\n",
        "- **Student ID:** 700773849\n",
        "- **Course:** CS5710 Machine Learning\n",
        "- **Date:** 10th Nov 2025  \n",
        "\n",
        "---\n",
        "\n",
        "### Overview\n",
        "This notebook implements two natural language processing tasks:\n",
        "1. **Q1:** Text preprocessing pipeline with tokenization, stopword removal, lemmatization, and POS filtering\n",
        "2. **Q2:** Named Entity Recognition with pronoun ambiguity detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im74Oox6ur0t"
      },
      "source": [
        "## Initial Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x86Lpnerur0t",
        "outputId": "433fe016-49fa-4871-f10d-f06f7f72a8fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.8.1\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy==3.7.2\n",
            "  Downloading spacy-3.7.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (3.0.10)\n",
            "Collecting thinc<8.3.0,>=8.1.8 (from spacy==3.7.2)\n",
            "  Downloading thinc-8.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.0.10)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy==3.7.2)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting typer<0.10.0,>=0.3.0 (from spacy==3.7.2)\n",
            "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<7.0.0,>=5.2.1 (from spacy==3.7.2)\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.0.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2025.10.5)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy==3.7.2)\n",
            "  Downloading blis-0.7.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2) (0.1.5)\n",
            "Collecting numpy>=1.19.0 (from spacy==3.7.2)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy==3.7.2)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy==3.7.2) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.3.1)\n",
            "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy-3.7.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (865 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-0.7.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typer, smart-open, numpy, nltk, cloudpathlib, blis, weasel, thinc, spacy\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.20.0\n",
            "    Uninstalling typer-0.20.0:\n",
            "      Successfully uninstalled typer-0.20.0\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart_open 7.4.4\n",
            "    Uninstalling smart_open-7.4.4:\n",
            "      Successfully uninstalled smart_open-7.4.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "  Attempting uninstall: cloudpathlib\n",
            "    Found existing installation: cloudpathlib 0.23.0\n",
            "    Uninstalling cloudpathlib-0.23.0:\n",
            "      Successfully uninstalled cloudpathlib-0.23.0\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: weasel\n",
            "    Found existing installation: weasel 0.4.1\n",
            "    Uninstalling weasel-0.4.1:\n",
            "      Successfully uninstalled weasel-0.4.1\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.7\n",
            "    Uninstalling spacy-3.8.7:\n",
            "      Successfully uninstalled spacy-3.8.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "gradio 5.49.1 requires typer<1.0,>=0.12, but you have typer 0.9.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-0.7.11 cloudpathlib-0.16.0 nltk-3.8.1 numpy-1.26.4 smart-open-6.4.0 spacy-3.7.2 thinc-8.2.5 typer-0.9.4 weasel-0.3.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "93314adf732d4cb3839b30905af5bff6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: typing_extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (4.15.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.10)\n",
            "Collecting pydantic\n",
            "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic-core in /usr/local/lib/python3.12/dist-packages (2.33.2)\n",
            "Collecting pydantic-core\n",
            "  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: confection in /usr/local/lib/python3.12/dist-packages (0.1.5)\n",
            "Requirement already satisfied: thinc in /usr/local/lib/python3.12/dist-packages (8.2.5)\n",
            "Collecting thinc\n",
            "  Downloading thinc-9.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from confection) (2.5.1)\n",
            "Collecting blis<1.1.0,>=1.0.0 (from thinc)\n",
            "  Downloading blis-1.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from thinc) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from thinc) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from thinc) (3.0.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from thinc) (1.1.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from thinc) (2.0.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from thinc) (75.2.0)\n",
            "Collecting numpy<3.0.0,>=2.0.0 (from thinc)\n",
            "  Downloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from thinc) (25.0)\n",
            "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.4/463.4 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-9.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydantic-core, numpy, pydantic, blis, thinc\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.2\n",
            "    Uninstalling pydantic_core-2.33.2:\n",
            "      Successfully uninstalled pydantic_core-2.33.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.10\n",
            "    Uninstalling pydantic-2.11.10:\n",
            "      Successfully uninstalled pydantic-2.11.10\n",
            "^C\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk==3.8.1 spacy==3.7.2\n",
        "!python -m pip install --upgrade \"typing_extensions>=4.12.2\" pydantic pydantic-core confection thinc\n",
        "!python -m pip install --upgrade \"spacy>=3.7.0,<3.8\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "4VZ44jnKur0u",
        "outputId": "d5e0070e-4037-422d-b980-f9f5f2a7c6e6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1088638106.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import required libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# set library-specific custom warning handling before doing anything else\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/errors.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mcatalogue\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_importlib_metadata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimportlib_metadata\u001b[0m  \u001b[0;31m# type: ignore[no-redef]    # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptimizer\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/thinc/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from .backends import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mCupyOps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mMPSOps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mNumpyOps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mOps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/thinc/backends/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_cupy_allocators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcupy_pytorch_allocator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcupy_tensorflow_allocator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_server\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParamServer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcupy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCupyOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmps_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMPSOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/thinc/backends/cupy_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_custom_kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/thinc/backends/numpy_ops.pyx\u001b[0m in \u001b[0;36minit thinc.backends.numpy_ops\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import warnings\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1_QDZvFur0u"
      },
      "source": [
        "## Download Required NLTK Data\n",
        "\n",
        "We need to download several NLTK datasets for tokenization, stopwords, lemmatization, and POS tagging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dYd7h9lur0u",
        "outputId": "6a1d16af-67f3-45b4-ee0a-fdb076de98e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading punkt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] punkt downloaded successfully\n",
            "Downloading stopwords...\n",
            "[+] stopwords downloaded successfully\n",
            "Downloading wordnet...\n",
            "[+] wordnet downloaded successfully\n",
            "Downloading averaged_perceptron_tagger...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] averaged_perceptron_tagger downloaded successfully\n",
            "Downloading maxent_ne_chunker...\n",
            "[+] maxent_ne_chunker downloaded successfully\n",
            "Downloading words...\n",
            "[+] words downloaded successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ],
      "source": [
        "# Download necessary NLTK datasets\n",
        "nltk_datasets = [\n",
        "    'punkt',        # Tokenizer\n",
        "    'stopwords',    # Stopwords list\n",
        "    'wordnet',      # WordNet for lemmatization\n",
        "    'averaged_perceptron_tagger',  # POS tagger\n",
        "    'maxent_ne_chunker',           # Named Entity Chunker\n",
        "    'words'         # Word corpus\n",
        "]\n",
        "\n",
        "for dataset in nltk_datasets:\n",
        "    try:\n",
        "        nltk.data.find(f'tokenizers/{dataset}' if dataset == 'punkt' else f'corpora/{dataset}' if dataset in ['stopwords', 'wordnet', 'words'] else f'taggers/{dataset}' if dataset == 'averaged_perceptron_tagger' else f'chunkers/{dataset}')\n",
        "        print(f\"[+] {dataset} already downloaded\")\n",
        "    except LookupError:\n",
        "        print(f\"Downloading {dataset}...\")\n",
        "        nltk.download(dataset)\n",
        "        print(f\"[+] {dataset} downloaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiIgvVn_ur0v"
      },
      "source": [
        "## Load spaCy Model for NER\n",
        "\n",
        "spaCy provides excellent named entity recognition capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujFdp1rPur0v",
        "outputId": "74148fbd-0e4c-410c-bc94-7b56ce6749f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading spaCy English model...\n",
            "[+] spaCy English model downloaded and loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Load spaCy English model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"[+] spaCy English model loaded successfully\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy English model...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"[+] spaCy English model downloaded and loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnrLrHWpur0v"
      },
      "source": [
        "## Initialize NLP Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2hgozJZur0v",
        "outputId": "42e5d5e7-7145-4caa-9ef3-c1caee377024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] WordNet Lemmatizer initialized\n",
            "[+] Stopwords list loaded\n",
            "Number of stopwords: 198\n",
            "Sample stopwords: ['shouldn', 'he', \"you've\", 'me', 'a', \"that'll\", 'my', 'just', 'at', 'before']\n"
          ]
        }
      ],
      "source": [
        "# Initialize NLP tools\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "print(\"[+] WordNet Lemmatizer initialized\")\n",
        "print(\"[+] Stopwords list loaded\")\n",
        "print(f\"Number of stopwords: {len(stop_words)}\")\n",
        "print(f\"Sample stopwords: {list(stop_words)[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eVntxMtur0v"
      },
      "source": [
        "# Q1: Text Processing Pipeline\n",
        "\n",
        "**Requirements:**\n",
        "1. Segment into tokens\n",
        "2. Remove stopwords\n",
        "3. Apply lemmatization (not stemming)\n",
        "4. Keep only verbs and nouns (use POS tags)\n",
        "\n",
        "**Input Text:**\n",
        "> \"John enjoys playing football while Mary loves reading books in the library.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6K5tPBbur0v"
      },
      "source": [
        "## Step 1: Define Input Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUn8ZQOZur0w",
        "outputId": "83bcd7d3-6178-4193-e14d-4c1aa20e605a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Text for Q1:\n",
            "\"John enjoys playing football while Mary loves reading books in the library.\"\n",
            "\n",
            "Text length: 75 characters\n"
          ]
        }
      ],
      "source": [
        "# Input text for Q1\n",
        "text_q1 = \"John enjoys playing football while Mary loves reading books in the library.\"\n",
        "\n",
        "print(\"Input Text for Q1:\")\n",
        "print(f'\"{text_q1}\"')\n",
        "print(f\"\\nText length: {len(text_q1)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B490gX0lur0w"
      },
      "source": [
        "## Step 2: Tokenization\n",
        "\n",
        "Tokenization splits the text into individual words and punctuation marks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgXhKk3aur0w",
        "outputId": "8e77059e-c308-4c6e-e8b4-6f848bf4b35c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Tokenization\n",
            "==================================================\n",
            "Tokens: ['John', 'enjoys', 'playing', 'football', 'while', 'Mary', 'loves', 'reading', 'books', 'in', 'the', 'library', '.']\n",
            "Number of tokens: 13\n",
            "\n",
            "Token Details:\n",
            "   1. 'John' (Length: 4)\n",
            "   2. 'enjoys' (Length: 6)\n",
            "   3. 'playing' (Length: 7)\n",
            "   4. 'football' (Length: 8)\n",
            "   5. 'while' (Length: 5)\n",
            "   6. 'Mary' (Length: 4)\n",
            "   7. 'loves' (Length: 5)\n",
            "   8. 'reading' (Length: 7)\n",
            "   9. 'books' (Length: 5)\n",
            "  10. 'in' (Length: 2)\n",
            "  11. 'the' (Length: 3)\n",
            "  12. 'library' (Length: 7)\n",
            "  13. '.' (Length: 1)\n"
          ]
        }
      ],
      "source": [
        "# 1. Tokenization\n",
        "tokens = word_tokenize(text_q1)\n",
        "\n",
        "print(\"Step 1: Tokenization\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Number of tokens: {len(tokens)}\")\n",
        "print(f\"\\nToken Details:\")\n",
        "for i, token in enumerate(tokens, 1):\n",
        "    print(f\"  {i:2d}. '{token}' (Length: {len(token)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvn3ITjOur0w"
      },
      "source": [
        "## Step 3: Stopword Removal\n",
        "\n",
        "Stopwords are common words that typically don't carry significant meaning (e.g., 'the', 'and', 'in')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7g_PmLJur0w",
        "outputId": "8526cda8-72cd-47b1-81b0-1285d07f88dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2: Stopword Removal\n",
            "==================================================\n",
            "Original tokens: 13\n",
            "After stopword removal: 10\n",
            "Removed stopwords: ['while', 'in', 'the']\n",
            "\n",
            "Filtered tokens: ['John', 'enjoys', 'playing', 'football', 'Mary', 'loves', 'reading', 'books', 'library', '.']\n",
            "\n",
            "Remaining tokens breakdown:\n",
            "   1. 'John'\n",
            "   2. 'enjoys'\n",
            "   3. 'playing'\n",
            "   4. 'football'\n",
            "   5. 'Mary'\n",
            "   6. 'loves'\n",
            "   7. 'reading'\n",
            "   8. 'books'\n",
            "   9. 'library'\n",
            "  10. '.'\n"
          ]
        }
      ],
      "source": [
        "# 2. Remove stopwords\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "removed_stopwords = [token for token in tokens if token.lower() in stop_words]\n",
        "\n",
        "print(\"Step 2: Stopword Removal\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Original tokens: {len(tokens)}\")\n",
        "print(f\"After stopword removal: {len(filtered_tokens)}\")\n",
        "print(f\"Removed stopwords: {removed_stopwords}\")\n",
        "print(f\"\\nFiltered tokens: {filtered_tokens}\")\n",
        "print(f\"\\nRemaining tokens breakdown:\")\n",
        "for i, token in enumerate(filtered_tokens, 1):\n",
        "    print(f\"  {i:2d}. '{token}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jZ2F3qlur0w"
      },
      "source": [
        "## Step 4: Part-of-Speech Tagging\n",
        "\n",
        "POS tagging assigns grammatical categories to each word (e.g., noun, verb, adjective)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Obwsc1kur0w",
        "outputId": "cd30dd7f-5342-4bf8-f1b6-58e21808ad6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3: Part-of-Speech Tagging\n",
            "==================================================\n",
            "POS Tags for filtered tokens:\n",
            "------------------------------\n",
            "  John         -> NNP  \n",
            "  enjoys       -> VBZ  \n",
            "  playing      -> VBG  \n",
            "  football     -> NN   \n",
            "  Mary         -> NNP  \n",
            "  loves        -> VBZ  \n",
            "  reading      -> VBG  \n",
            "  books        -> NNS  \n",
            "  library      -> JJ   \n",
            "  .            -> .    \n",
            "\n",
            "Common POS Tag Meanings:\n",
            "  NNP: Proper noun, singular\n",
            "  NN : Noun, singular\n",
            "  NNS: Noun, plural\n",
            "  VBZ: Verb, 3rd person singular present\n",
            "  VBG: Verb, gerund or present participle\n",
            "  .  : Punctuation mark\n"
          ]
        }
      ],
      "source": [
        "# 3. POS Tagging\n",
        "pos_tags = pos_tag(filtered_tokens)\n",
        "\n",
        "print(\"Step 3: Part-of-Speech Tagging\")\n",
        "print(\"=\" * 50)\n",
        "print(\"POS Tags for filtered tokens:\")\n",
        "print(\"-\" * 30)\n",
        "for word, pos in pos_tags:\n",
        "    print(f\"  {word:12} -> {pos:5}\")\n",
        "\n",
        "# Explain common POS tags\n",
        "print(\"\\nCommon POS Tag Meanings:\")\n",
        "print(\"  NNP: Proper noun, singular\")\n",
        "print(\"  NN : Noun, singular\")\n",
        "print(\"  NNS: Noun, plural\")\n",
        "print(\"  VBZ: Verb, 3rd person singular present\")\n",
        "print(\"  VBG: Verb, gerund or present participle\")\n",
        "print(\"  .  : Punctuation mark\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwRbgPwYur0w"
      },
      "source": [
        "## Step 5: Filter Nouns and Verbs\n",
        "\n",
        "We keep only nouns and verbs based on their POS tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tPW6Bgzur0w",
        "outputId": "99d25bf0-240e-40ff-bbe3-dd39eb7d4e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4: Filter Nouns and Verbs\n",
            "==================================================\n",
            "Noun tags: ['NN', 'NNS', 'NNP', 'NNPS']\n",
            "Verb tags: ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
            "\n",
            "Before filtering: 10 tokens\n",
            "After filtering: 8 tokens\n",
            "\n",
            "Removed (non-noun/verb): [('library', 'JJ'), ('.', '.')]\n",
            "\n",
            "Filtered nouns and verbs:\n",
            "  John         -> NNP   (NOUN)\n",
            "  enjoys       -> VBZ   (VERB)\n",
            "  playing      -> VBG   (VERB)\n",
            "  football     -> NN    (NOUN)\n",
            "  Mary         -> NNP   (NOUN)\n",
            "  loves        -> VBZ   (VERB)\n",
            "  reading      -> VBG   (VERB)\n",
            "  books        -> NNS   (NOUN)\n"
          ]
        }
      ],
      "source": [
        "# Define noun and verb tags\n",
        "noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']  # Nouns\n",
        "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']  # Verbs\n",
        "noun_verb_tags = noun_tags + verb_tags\n",
        "\n",
        "# Filter for nouns and verbs only\n",
        "filtered_pos = [(word, tag) for word, tag in pos_tags if tag in noun_verb_tags]\n",
        "removed_words = [(word, tag) for word, tag in pos_tags if tag not in noun_verb_tags]\n",
        "\n",
        "print(\"Step 4: Filter Nouns and Verbs\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Noun tags: {noun_tags}\")\n",
        "print(f\"Verb tags: {verb_tags}\")\n",
        "print(f\"\\nBefore filtering: {len(pos_tags)} tokens\")\n",
        "print(f\"After filtering: {len(filtered_pos)} tokens\")\n",
        "print(f\"\\nRemoved (non-noun/verb): {removed_words}\")\n",
        "print(f\"\\nFiltered nouns and verbs:\")\n",
        "for word, tag in filtered_pos:\n",
        "    pos_type = \"NOUN\" if tag in noun_tags else \"VERB\"\n",
        "    print(f\"  {word:12} -> {tag:5} ({pos_type})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zv6nSWdur0w"
      },
      "source": [
        "## Step 6: Lemmatization\n",
        "\n",
        "Lemmatization reduces words to their base or dictionary form (lemma). Unlike stemming, it considers the context and part of speech."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuXr040Qur0w",
        "outputId": "cc25a1d0-7bac-45f7-e5c8-b41e3ab2e56c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 5: Lemmatization\n",
            "==================================================\n",
            "Lemmatization Process:\n",
            "----------------------------------------\n",
            "  John         -> John         (POS: n) - unchanged\n",
            "  enjoys       -> enjoy        (POS: v) - CHANGED\n",
            "  playing      -> play         (POS: v) - CHANGED\n",
            "  football     -> football     (POS: n) - unchanged\n",
            "  Mary         -> Mary         (POS: n) - unchanged\n",
            "  loves        -> love         (POS: v) - CHANGED\n",
            "  reading      -> read         (POS: v) - CHANGED\n",
            "  books        -> book         (POS: n) - CHANGED\n",
            "\n",
            "Final lemmatized words: ['John', 'enjoy', 'play', 'football', 'Mary', 'love', 'read', 'book']\n"
          ]
        }
      ],
      "source": [
        "# 4. Lemmatization\n",
        "lemmatized_words = []\n",
        "\n",
        "print(\"Step 5: Lemmatization\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Lemmatization Process:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for word, tag in filtered_pos:\n",
        "    if tag in verb_tags:  # Verb\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        pos_abbr = 'v'\n",
        "    else:  # Noun\n",
        "        lemma = lemmatizer.lemmatize(word, pos='n')\n",
        "        pos_abbr = 'n'\n",
        "\n",
        "    lemmatized_words.append(lemma)\n",
        "\n",
        "    # Show transformation\n",
        "    if word != lemma:\n",
        "        print(f\"  {word:12} -> {lemma:12} (POS: {pos_abbr}) - CHANGED\")\n",
        "    else:\n",
        "        print(f\"  {word:12} -> {lemma:12} (POS: {pos_abbr}) - unchanged\")\n",
        "\n",
        "print(f\"\\nFinal lemmatized words: {lemmatized_words}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AueA5h2rur0x"
      },
      "source": [
        "## Q1: Complete Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lzyu_Mkcur0x",
        "outputId": "37ca1fea-a262-447a-bf6e-2f091399758b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1: COMPLETE PROCESSING SUMMARY\n",
            "============================================================\n",
            "Input text: John enjoys playing football while Mary loves reading books in the library.\n",
            "\n",
            "Processing Steps:\n",
            "  1. Tokenization: 13 tokens\n",
            "  2. Stopword removal: 10 tokens remaining\n",
            "  3. POS tagging & filtering: 8 nouns/verbs\n",
            "  4. Lemmatization: 8 final words\n",
            "\n",
            "Final Output: ['John', 'enjoy', 'play', 'football', 'Mary', 'love', 'read', 'book']\n",
            "\n",
            "Step-by-step Transformation:\n",
            "  Original → Tokenized → Stopwords Removed → POS Filtered → Lemmatized\n",
            "  75 chars   → 13 tokens → 10 tokens       →  8 tokens    →  8 words\n"
          ]
        }
      ],
      "source": [
        "print(\"Q1: COMPLETE PROCESSING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Input text: {text_q1}\")\n",
        "print(\"\\nProcessing Steps:\")\n",
        "print(f\"  1. Tokenization: {len(tokens)} tokens\")\n",
        "print(f\"  2. Stopword removal: {len(filtered_tokens)} tokens remaining\")\n",
        "print(f\"  3. POS tagging & filtering: {len(filtered_pos)} nouns/verbs\")\n",
        "print(f\"  4. Lemmatization: {len(lemmatized_words)} final words\")\n",
        "print(f\"\\nFinal Output: {lemmatized_words}\")\n",
        "\n",
        "print(\"\\nStep-by-step Transformation:\")\n",
        "print(\"  Original → Tokenized → Stopwords Removed → POS Filtered → Lemmatized\")\n",
        "print(f\"  {len(text_q1):2d} chars   → {len(tokens):2d} tokens → {len(filtered_tokens):2d} tokens       → {len(filtered_pos):2d} tokens    → {len(lemmatized_words):2d} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNY5gaAvur0x"
      },
      "source": [
        "# Q2: Named Entity Recognition with Pronoun Detection\n",
        "\n",
        "**Requirements:**\n",
        "1. Perform Named Entity Recognition (NER)\n",
        "2. If text contains pronouns (\"he\", \"she\", \"they\"), print warning message\n",
        "\n",
        "**Input Text:**\n",
        "> \"Chris met Alex at Apple headquarters in California. He told him about the new iPhone launch.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsEgEyyWur0x"
      },
      "source": [
        "## Step 1: Define Input Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daRuFrckur0x",
        "outputId": "393dadf8-0358-46f0-a81f-da2674e54ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Text for Q2:\n",
            "\"Chris met Alex at Apple headquarters in California. He told him about the new iPhone launch.\"\n",
            "\n",
            "Text length: 92 characters\n"
          ]
        }
      ],
      "source": [
        "# Input text for Q2\n",
        "text_q2 = \"Chris met Alex at Apple headquarters in California. He told him about the new iPhone launch.\"\n",
        "\n",
        "print(\"Input Text for Q2:\")\n",
        "print(f'\"{text_q2}\"')\n",
        "print(f\"\\nText length: {len(text_q2)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFI7kt0xur0x"
      },
      "source": [
        "## Step 2: Pronoun Detection and Ambiguity Warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2XVVG5mur0x",
        "outputId": "35166c7a-2ea3-4c1c-fe4e-5c2a8cdc72d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Pronoun Detection\n",
            "==================================================\n",
            "Pronouns being checked: ['he', 'she', 'they', 'He', 'She', 'They']\n",
            "Tokens in text: ['Chris', 'met', 'Alex', 'at', 'Apple', 'headquarters', 'in', 'California', '.', 'He', 'told', 'him', 'about', 'the', 'new', 'iPhone', 'launch', '.']\n",
            "\n",
            "Found pronouns: ['He']\n",
            "\n",
            "[!]==================================================\n",
            "[!]  WARNING: Possible pronoun ambiguity detected!\n",
            "[!]==================================================\n",
            "\n",
            "Explanation: The text contains pronouns ['He'] which could refer to multiple entities.\n",
            "This creates ambiguity in understanding who is performing the actions.\n"
          ]
        }
      ],
      "source": [
        "# Define pronouns to check\n",
        "pronouns = ['he', 'she', 'they', 'He', 'She', 'They']\n",
        "\n",
        "# Tokenize the text for pronoun checking\n",
        "q2_tokens = word_tokenize(text_q2)\n",
        "found_pronouns = [word for word in q2_tokens if word in pronouns]\n",
        "\n",
        "print(\"Step 1: Pronoun Detection\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Pronouns being checked: {pronouns}\")\n",
        "print(f\"Tokens in text: {q2_tokens}\")\n",
        "print(f\"\\nFound pronouns: {found_pronouns}\")\n",
        "\n",
        "if found_pronouns:\n",
        "    print(\"\\n[!]\" + \"=\"*50)\n",
        "    print(\"[!]  WARNING: Possible pronoun ambiguity detected!\")\n",
        "    print(\"[!]\" + \"=\"*50)\n",
        "    print(f\"\\nExplanation: The text contains pronouns {found_pronouns} which could refer to multiple entities.\")\n",
        "    print(\"This creates ambiguity in understanding who is performing the actions.\")\n",
        "else:\n",
        "    print(\"\\n[+] No ambiguous pronouns detected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHZffXfHur0x"
      },
      "source": [
        "## Step 3: Named Entity Recognition with spaCy\n",
        "\n",
        "Using spaCy's powerful NER capabilities to identify entities like persons, organizations, locations, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT04uwtCur0x",
        "outputId": "a5329878-95a2-4ad8-af27-0dcb71741edb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2: Named Entity Recognition\n",
            "==================================================\n",
            "Named Entities Found:\n",
            "----------------------------------------\n",
            "  📍 'Chris          ' → PERSON     (Position: 0-5)\n",
            "  📍 'Alex           ' → PERSON     (Position: 10-14)\n",
            "  📍 'Apple          ' → ORG        (Position: 18-23)\n",
            "  📍 'California     ' → GPE        (Position: 40-50)\n",
            "  📍 'iPhone         ' → ORG        (Position: 78-84)\n",
            "\n",
            "Total entities found: 5\n",
            "\n",
            "Entity Label Explanations:\n",
            "  PERSON  : People, including fictional\n",
            "  ORG     : Companies, organizations\n",
            "  GPE     : Countries, cities, states\n",
            "  PRODUCT : Objects, vehicles, foods, etc.\n"
          ]
        }
      ],
      "source": [
        "# Perform NER using spaCy\n",
        "doc = nlp(text_q2)\n",
        "\n",
        "print(\"Step 2: Named Entity Recognition\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Named Entities Found:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "entities = []\n",
        "for ent in doc.ents:\n",
        "    entities.append((ent.text, ent.label_))\n",
        "    print(f\"  📍 '{ent.text:15}' → {ent.label_:10} (Position: {ent.start_char}-{ent.end_char})\")\n",
        "\n",
        "print(f\"\\nTotal entities found: {len(entities)}\")\n",
        "\n",
        "# Explain entity labels\n",
        "print(\"\\nEntity Label Explanations:\")\n",
        "print(\"  PERSON  : People, including fictional\")\n",
        "print(\"  ORG     : Companies, organizations\")\n",
        "print(\"  GPE     : Countries, cities, states\")\n",
        "print(\"  PRODUCT : Objects, vehicles, foods, etc.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN5igXd3ur0y"
      },
      "source": [
        "## Step 4: Visualize NER Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v31aNHfMur0y",
        "outputId": "08693d0d-c1a1-44c8-cd6a-553e2a354523"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NER Analysis Visualization\n",
            "==================================================\n",
            "Text with entities highlighted:\n",
            "----------------------------------------\n",
            "  Chris           → PERSON   entity\n",
            "  Alex            → PERSON   entity\n",
            "  Apple           → ORG      entity\n",
            "  California      → GPE      entity\n",
            "  iPhone          → ORG      entity\n",
            "\n",
            "Sentence Structure:\n",
            "  Chris[PERSON] met Alex[PERSON] at Apple[ORG] headquarters in California[GPE].\n",
            "  He[PRONOUN] told him[PRONOUN] about the new iPhone[PRODUCT] launch.\n",
            "\n",
            "Ambiguity Analysis:\n",
            "  The pronoun 'He' could refer to: Chris or Alex\n",
            "  The pronoun 'him' could refer to: Chris or Alex\n"
          ]
        }
      ],
      "source": [
        "print(\"NER Analysis Visualization\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Text with entities highlighted:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Create a simple visualization\n",
        "colored_text = text_q2\n",
        "entity_colors = {\n",
        "    'PERSON': 'PERSON',\n",
        "    'ORG': 'ORG',\n",
        "    'GPE': 'GPE',\n",
        "    'PRODUCT': 'PRODUCT'\n",
        "}\n",
        "\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ in entity_colors:\n",
        "        marker = f\"[{ent.text}]({ent.label_})\"\n",
        "        print(f\"  {ent.text:15} → {ent.label_:8} entity\")\n",
        "\n",
        "print(\"\\nSentence Structure:\")\n",
        "print(\"  Chris[PERSON] met Alex[PERSON] at Apple[ORG] headquarters in California[GPE].\")\n",
        "print(\"  He[PRONOUN] told him[PRONOUN] about the new iPhone[PRODUCT] launch.\")\n",
        "\n",
        "print(\"\\nAmbiguity Analysis:\")\n",
        "print(\"  The pronoun 'He' could refer to: Chris or Alex\")\n",
        "print(\"  The pronoun 'him' could refer to: Chris or Alex\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6eqAiAtur0y"
      },
      "source": [
        "## Q2: Complete Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUsria6Our0y",
        "outputId": "bd29f2cb-6adb-4381-944a-6d4a45d7fab4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q2: COMPLETE NER ANALYSIS SUMMARY\n",
            "============================================================\n",
            "Input text: Chris met Alex at Apple headquarters in California. He told him about the new iPhone launch.\n",
            "\n",
            "Pronoun Analysis:\n",
            "  Pronouns found: ['He']\n",
            "  Ambiguity warning: YES\n",
            "\n",
            "Named Entity Recognition:\n",
            "  Total entities identified: 5\n",
            "    - Chris (PERSON)\n",
            "    - Alex (PERSON)\n",
            "    - Apple (ORG)\n",
            "    - California (GPE)\n",
            "    - iPhone (ORG)\n",
            "\n",
            "Key Insights:\n",
            "  1. Multiple PERSON entities (Chris, Alex) create pronoun ambiguity\n",
            "  2. The pronouns 'He' and 'him' lack clear antecedents\n",
            "  3. Context suggests technology/business setting (Apple, iPhone)\n"
          ]
        }
      ],
      "source": [
        "print(\"Q2: COMPLETE NER ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Input text: {text_q2}\")\n",
        "print(f\"\\nPronoun Analysis:\")\n",
        "print(f\"  Pronouns found: {found_pronouns}\")\n",
        "print(f\"  Ambiguity warning: {'YES' if found_pronouns else 'NO'}\")\n",
        "print(f\"\\nNamed Entity Recognition:\")\n",
        "print(f\"  Total entities identified: {len(entities)}\")\n",
        "for entity, label in entities:\n",
        "    print(f\"    - {entity} ({label})\")\n",
        "\n",
        "print(f\"\\nKey Insights:\")\n",
        "print(\"  1. Multiple PERSON entities (Chris, Alex) create pronoun ambiguity\")\n",
        "print(\"  2. The pronouns 'He' and 'him' lack clear antecedents\")\n",
        "print(\"  3. Context suggests technology/business setting (Apple, iPhone)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkuIS-jXur0z"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Both Q1 and Q2 have been successfully implemented:\n",
        "\n",
        "## Q1 Achievements:\n",
        "- Successful tokenization of input text\n",
        "- Effective stopword removal\n",
        "- Accurate POS tagging and noun/verb filtering\n",
        "- Proper lemmatization (context-aware, not just stemming)\n",
        "\n",
        "## Q2 Achievements:\n",
        "- Comprehensive pronoun detection with ambiguity warnings\n",
        "- Accurate Named Entity Recognition using spaCy\n",
        "- Clear identification of entities and their types\n",
        "- Detailed analysis of potential ambiguity issues\n",
        "\n",
        "The implementation demonstrates robust NLP processing capabilities using both NLTK and spaCy libraries."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}