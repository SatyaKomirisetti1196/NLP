{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Processing Assignment\n",
    "## Part C - Coding Questions\n",
    "\n",
    "- **Student Name:** Satya Komirisetti\n",
    "- **Student ID:** 700773849\n",
    "- **Course:** CS5710 Machine Learning\n",
    "- **Semester:** Fall 2025\n",
    "- **University:** University of Central Missouri\n",
    "- **Date:** 10th Nov 2025  \n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "This notebook implements two natural language processing tasks:\n",
    "1. **Q1:** Text preprocessing pipeline with tokenization, stopword removal, lemmatization, and POS filtering\n",
    "2. **Q2:** Named Entity Recognition with pronoun ambiguity detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.8.1 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.8.1)\n",
      "Collecting spacy==3.7.2\n",
      "  Using cached spacy-3.7.2-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "Requirement already satisfied: click in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk==3.8.1) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk==3.8.1) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk==3.8.1) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk==3.8.1) (4.67.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (6.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from spacy==3.7.2) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy==3.7.2) (2.3.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy==3.7.2)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->nltk==3.8.1) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.2) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->spacy==3.7.2) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.3.1)\n",
      "Installing collected packages: numpy, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.4\n",
      "    Uninstalling numpy-2.3.4:\n",
      "      Successfully uninstalled numpy-2.3.4\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.7.5\n",
      "    Uninstalling spacy-3.7.5:\n",
      "      Successfully uninstalled spacy-3.7.5\n",
      "Successfully installed numpy-1.26.4 spacy-3.7.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\VAMSI\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions>=4.12.2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.15.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.12.4)\n",
      "Requirement already satisfied: pydantic-core in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.41.5)\n",
      "Requirement already satisfied: confection in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.1.5)\n",
      "Requirement already satisfied: thinc in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (8.2.5)\n",
      "Collecting thinc\n",
      "  Using cached thinc-9.1.1-cp311-cp311-win_amd64.whl (1.3 MB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from confection) (2.5.1)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc)\n",
      "  Using cached blis-1.0.2-cp311-cp311-win_amd64.whl (6.3 MB)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=1.0.2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc) (3.0.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.8.1 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc) (1.1.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc) (2.0.10)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from thinc) (65.5.0)\n",
      "Collecting numpy<3.0.0,>=2.0.0 (from thinc)\n",
      "  Using cached numpy-2.3.4-cp311-cp311-win_amd64.whl (13.1 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc) (24.1)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from wasabi<1.2.0,>=0.8.1->thinc) (0.4.6)\n",
      "Installing collected packages: numpy, blis, thinc\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: blis\n",
      "    Found existing installation: blis 0.7.11\n",
      "    Uninstalling blis-0.7.11:\n",
      "      Successfully uninstalled blis-0.7.11\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.2.5\n",
      "    Uninstalling thinc-8.2.5:\n",
      "      Successfully uninstalled thinc-8.2.5\n",
      "Successfully installed blis-1.0.2 numpy-2.3.4 thinc-9.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.7.2 requires thinc<8.3.0,>=8.1.8, but you have thinc 9.1.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\VAMSI\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy<3.8,>=3.7.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.7.2)\n",
      "Collecting spacy<3.8,>=3.7.0\n",
      "  Using cached spacy-3.7.5-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (3.0.10)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy<3.8,>=3.7.0)\n",
      "  Using cached thinc-8.2.5-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (0.9.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from spacy<3.8,>=3.7.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy<3.8,>=3.7.0) (2.3.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8,>=3.7.0) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8,>=3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8,>=3.7.0) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8,>=3.7.0) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8,>=3.7.0) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8,>=3.7.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8,>=3.7.0) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8,>=3.7.0) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8,>=3.7.0) (2023.5.7)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy<3.8,>=3.7.0)\n",
      "  Using cached blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8,>=3.7.0) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy<3.8,>=3.7.0)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8,>=3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8,>=3.7.0) (8.1.8)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8,>=3.7.0) (0.16.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8,>=3.7.0) (6.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->spacy<3.8,>=3.7.0) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\vamsi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8,>=3.7.0) (1.3.1)\n",
      "Installing collected packages: numpy, blis, thinc, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.4\n",
      "    Uninstalling numpy-2.3.4:\n",
      "      Successfully uninstalled numpy-2.3.4\n",
      "  Attempting uninstall: blis\n",
      "    Found existing installation: blis 1.0.2\n",
      "    Uninstalling blis-1.0.2:\n",
      "      Successfully uninstalled blis-1.0.2\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 9.1.1\n",
      "    Uninstalling thinc-9.1.1:\n",
      "      Successfully uninstalled thinc-9.1.1\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.7.2\n",
      "    Uninstalling spacy-3.7.2:\n",
      "      Successfully uninstalled spacy-3.7.2\n",
      "Successfully installed blis-0.7.11 numpy-1.26.4 spacy-3.7.5 thinc-8.2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\VAMSI\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.8.1 spacy==3.7.2\n",
    "!python -m pip install --upgrade \"typing_extensions>=4.12.2\" pydantic pydantic-core confection thinc\n",
    "!python -m pip install --upgrade \"spacy>=3.7.0,<3.8\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import warnings\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Required NLTK Data\n",
    "\n",
    "We need to download several NLTK datasets for tokenization, stopwords, lemmatization, and POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading punkt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VAMSI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] punkt downloaded successfully\n",
      "Downloading stopwords...\n",
      "[+] stopwords downloaded successfully\n",
      "Downloading wordnet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VAMSI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\VAMSI\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] wordnet downloaded successfully\n",
      "Downloading averaged_perceptron_tagger...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\VAMSI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\VAMSI\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] averaged_perceptron_tagger downloaded successfully\n",
      "Downloading maxent_ne_chunker...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] maxent_ne_chunker downloaded successfully\n",
      "Downloading words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\VAMSI\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] words downloaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK datasets\n",
    "nltk_datasets = [\n",
    "    'punkt',        # Tokenizer\n",
    "    'stopwords',    # Stopwords list\n",
    "    'wordnet',      # WordNet for lemmatization\n",
    "    'averaged_perceptron_tagger',  # POS tagger\n",
    "    'maxent_ne_chunker',           # Named Entity Chunker\n",
    "    'words'         # Word corpus\n",
    "]\n",
    "\n",
    "for dataset in nltk_datasets:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{dataset}' if dataset == 'punkt' else f'corpora/{dataset}' if dataset in ['stopwords', 'wordnet', 'words'] else f'taggers/{dataset}' if dataset == 'averaged_perceptron_tagger' else f'chunkers/{dataset}')\n",
    "        print(f\"[+] {dataset} already downloaded\")\n",
    "    except LookupError:\n",
    "        print(f\"Downloading {dataset}...\")\n",
    "        nltk.download(dataset)\n",
    "        print(f\"[+] {dataset} downloaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spaCy Model for NER\n",
    "\n",
    "spaCy provides excellent named entity recognition capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading spaCy English model...\n",
      "[+] spaCy English model downloaded and loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy English model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"[+] spaCy English model loaded successfully\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy English model...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"[+] spaCy English model downloaded and loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize NLP Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] WordNet Lemmatizer initialized\n",
      "[+] Stopwords list loaded\n",
      "Number of stopwords: 198\n",
      "Sample stopwords: ['shouldn', 'he', \"you've\", 'me', 'a', \"that'll\", 'my', 'just', 'at', 'before']\n"
     ]
    }
   ],
   "source": [
    "# Initialize NLP tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"[+] WordNet Lemmatizer initialized\")\n",
    "print(\"[+] Stopwords list loaded\")\n",
    "print(f\"Number of stopwords: {len(stop_words)}\")\n",
    "print(f\"Sample stopwords: {list(stop_words)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Text Processing Pipeline\n",
    "\n",
    "**Requirements:**\n",
    "1. Segment into tokens\n",
    "2. Remove stopwords\n",
    "3. Apply lemmatization (not stemming)\n",
    "4. Keep only verbs and nouns (use POS tags)\n",
    "\n",
    "**Input Text:**\n",
    "> \"John enjoys playing football while Mary loves reading books in the library.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text for Q1:\n",
      "\"John enjoys playing football while Mary loves reading books in the library.\"\n",
      "\n",
      "Text length: 75 characters\n"
     ]
    }
   ],
   "source": [
    "# Input text for Q1\n",
    "text_q1 = \"John enjoys playing football while Mary loves reading books in the library.\"\n",
    "\n",
    "print(\"Input Text for Q1:\")\n",
    "print(f'\"{text_q1}\"')\n",
    "print(f\"\\nText length: {len(text_q1)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tokenization\n",
    "\n",
    "Tokenization splits the text into individual words and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Tokenization\n",
      "==================================================\n",
      "Tokens: ['John', 'enjoys', 'playing', 'football', 'while', 'Mary', 'loves', 'reading', 'books', 'in', 'the', 'library', '.']\n",
      "Number of tokens: 13\n",
      "\n",
      "Token Details:\n",
      "   1. 'John' (Length: 4)\n",
      "   2. 'enjoys' (Length: 6)\n",
      "   3. 'playing' (Length: 7)\n",
      "   4. 'football' (Length: 8)\n",
      "   5. 'while' (Length: 5)\n",
      "   6. 'Mary' (Length: 4)\n",
      "   7. 'loves' (Length: 5)\n",
      "   8. 'reading' (Length: 7)\n",
      "   9. 'books' (Length: 5)\n",
      "  10. 'in' (Length: 2)\n",
      "  11. 'the' (Length: 3)\n",
      "  12. 'library' (Length: 7)\n",
      "  13. '.' (Length: 1)\n"
     ]
    }
   ],
   "source": [
    "# 1. Tokenization\n",
    "tokens = word_tokenize(text_q1)\n",
    "\n",
    "print(\"Step 1: Tokenization\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"\\nToken Details:\")\n",
    "for i, token in enumerate(tokens, 1):\n",
    "    print(f\"  {i:2d}. '{token}' (Length: {len(token)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Stopword Removal\n",
    "\n",
    "Stopwords are common words that typically don't carry significant meaning (e.g., 'the', 'and', 'in')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Stopword Removal\n",
      "==================================================\n",
      "Original tokens: 13\n",
      "After stopword removal: 10\n",
      "Removed stopwords: ['while', 'in', 'the']\n",
      "\n",
      "Filtered tokens: ['John', 'enjoys', 'playing', 'football', 'Mary', 'loves', 'reading', 'books', 'library', '.']\n",
      "\n",
      "Remaining tokens breakdown:\n",
      "   1. 'John'\n",
      "   2. 'enjoys'\n",
      "   3. 'playing'\n",
      "   4. 'football'\n",
      "   5. 'Mary'\n",
      "   6. 'loves'\n",
      "   7. 'reading'\n",
      "   8. 'books'\n",
      "   9. 'library'\n",
      "  10. '.'\n"
     ]
    }
   ],
   "source": [
    "# 2. Remove stopwords\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "removed_stopwords = [token for token in tokens if token.lower() in stop_words]\n",
    "\n",
    "print(\"Step 2: Stopword Removal\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original tokens: {len(tokens)}\")\n",
    "print(f\"After stopword removal: {len(filtered_tokens)}\")\n",
    "print(f\"Removed stopwords: {removed_stopwords}\")\n",
    "print(f\"\\nFiltered tokens: {filtered_tokens}\")\n",
    "print(f\"\\nRemaining tokens breakdown:\")\n",
    "for i, token in enumerate(filtered_tokens, 1):\n",
    "    print(f\"  {i:2d}. '{token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Part-of-Speech Tagging\n",
    "\n",
    "POS tagging assigns grammatical categories to each word (e.g., noun, verb, adjective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Part-of-Speech Tagging\n",
      "==================================================\n",
      "POS Tags for filtered tokens:\n",
      "------------------------------\n",
      "  John         -> NNP  \n",
      "  enjoys       -> VBZ  \n",
      "  playing      -> VBG  \n",
      "  football     -> NN   \n",
      "  Mary         -> NNP  \n",
      "  loves        -> VBZ  \n",
      "  reading      -> VBG  \n",
      "  books        -> NNS  \n",
      "  library      -> JJ   \n",
      "  .            -> .    \n",
      "\n",
      "Common POS Tag Meanings:\n",
      "  NNP: Proper noun, singular\n",
      "  NN : Noun, singular\n",
      "  NNS: Noun, plural\n",
      "  VBZ: Verb, 3rd person singular present\n",
      "  VBG: Verb, gerund or present participle\n",
      "  .  : Punctuation mark\n"
     ]
    }
   ],
   "source": [
    "# 3. POS Tagging\n",
    "pos_tags = pos_tag(filtered_tokens)\n",
    "\n",
    "print(\"Step 3: Part-of-Speech Tagging\")\n",
    "print(\"=\" * 50)\n",
    "print(\"POS Tags for filtered tokens:\")\n",
    "print(\"-\" * 30)\n",
    "for word, pos in pos_tags:\n",
    "    print(f\"  {word:12} -> {pos:5}\")\n",
    "    \n",
    "# Explain common POS tags\n",
    "print(\"\\nCommon POS Tag Meanings:\")\n",
    "print(\"  NNP: Proper noun, singular\")\n",
    "print(\"  NN : Noun, singular\")\n",
    "print(\"  NNS: Noun, plural\") \n",
    "print(\"  VBZ: Verb, 3rd person singular present\")\n",
    "print(\"  VBG: Verb, gerund or present participle\")\n",
    "print(\"  .  : Punctuation mark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Filter Nouns and Verbs\n",
    "\n",
    "We keep only nouns and verbs based on their POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Filter Nouns and Verbs\n",
      "==================================================\n",
      "Noun tags: ['NN', 'NNS', 'NNP', 'NNPS']\n",
      "Verb tags: ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
      "\n",
      "Before filtering: 10 tokens\n",
      "After filtering: 8 tokens\n",
      "\n",
      "Removed (non-noun/verb): [('library', 'JJ'), ('.', '.')]\n",
      "\n",
      "Filtered nouns and verbs:\n",
      "  John         -> NNP   (NOUN)\n",
      "  enjoys       -> VBZ   (VERB)\n",
      "  playing      -> VBG   (VERB)\n",
      "  football     -> NN    (NOUN)\n",
      "  Mary         -> NNP   (NOUN)\n",
      "  loves        -> VBZ   (VERB)\n",
      "  reading      -> VBG   (VERB)\n",
      "  books        -> NNS   (NOUN)\n"
     ]
    }
   ],
   "source": [
    "# Define noun and verb tags\n",
    "noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']  # Nouns\n",
    "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']  # Verbs\n",
    "noun_verb_tags = noun_tags + verb_tags\n",
    "\n",
    "# Filter for nouns and verbs only\n",
    "filtered_pos = [(word, tag) for word, tag in pos_tags if tag in noun_verb_tags]\n",
    "removed_words = [(word, tag) for word, tag in pos_tags if tag not in noun_verb_tags]\n",
    "\n",
    "print(\"Step 4: Filter Nouns and Verbs\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Noun tags: {noun_tags}\")\n",
    "print(f\"Verb tags: {verb_tags}\")\n",
    "print(f\"\\nBefore filtering: {len(pos_tags)} tokens\")\n",
    "print(f\"After filtering: {len(filtered_pos)} tokens\")\n",
    "print(f\"\\nRemoved (non-noun/verb): {removed_words}\")\n",
    "print(f\"\\nFiltered nouns and verbs:\")\n",
    "for word, tag in filtered_pos:\n",
    "    pos_type = \"NOUN\" if tag in noun_tags else \"VERB\"\n",
    "    print(f\"  {word:12} -> {tag:5} ({pos_type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base or dictionary form (lemma). Unlike stemming, it considers the context and part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Lemmatization\n",
      "==================================================\n",
      "Lemmatization Process:\n",
      "----------------------------------------\n",
      "  John         -> John         (POS: n) - unchanged\n",
      "  enjoys       -> enjoy        (POS: v) - CHANGED\n",
      "  playing      -> play         (POS: v) - CHANGED\n",
      "  football     -> football     (POS: n) - unchanged\n",
      "  Mary         -> Mary         (POS: n) - unchanged\n",
      "  loves        -> love         (POS: v) - CHANGED\n",
      "  reading      -> read         (POS: v) - CHANGED\n",
      "  books        -> book         (POS: n) - CHANGED\n",
      "\n",
      "Final lemmatized words: ['John', 'enjoy', 'play', 'football', 'Mary', 'love', 'read', 'book']\n"
     ]
    }
   ],
   "source": [
    "# 4. Lemmatization\n",
    "lemmatized_words = []\n",
    "\n",
    "print(\"Step 5: Lemmatization\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Lemmatization Process:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for word, tag in filtered_pos:\n",
    "    if tag in verb_tags:  # Verb\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        pos_abbr = 'v'\n",
    "    else:  # Noun\n",
    "        lemma = lemmatizer.lemmatize(word, pos='n')\n",
    "        pos_abbr = 'n'\n",
    "    \n",
    "    lemmatized_words.append(lemma)\n",
    "    \n",
    "    # Show transformation\n",
    "    if word != lemma:\n",
    "        print(f\"  {word:12} -> {lemma:12} (POS: {pos_abbr}) - CHANGED\")\n",
    "    else:\n",
    "        print(f\"  {word:12} -> {lemma:12} (POS: {pos_abbr}) - unchanged\")\n",
    "\n",
    "print(f\"\\nFinal lemmatized words: {lemmatized_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Complete Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: COMPLETE PROCESSING SUMMARY\n",
      "============================================================\n",
      "Input text: John enjoys playing football while Mary loves reading books in the library.\n",
      "\n",
      "Processing Steps:\n",
      "  1. Tokenization: 13 tokens\n",
      "  2. Stopword removal: 10 tokens remaining\n",
      "  3. POS tagging & filtering: 8 nouns/verbs\n",
      "  4. Lemmatization: 8 final words\n",
      "\n",
      "Final Output: ['John', 'enjoy', 'play', 'football', 'Mary', 'love', 'read', 'book']\n",
      "\n",
      "Step-by-step Transformation:\n",
      "  Original → Tokenized → Stopwords Removed → POS Filtered → Lemmatized\n",
      "  75 chars   → 13 tokens → 10 tokens       →  8 tokens    →  8 words\n"
     ]
    }
   ],
   "source": [
    "print(\"Q1: COMPLETE PROCESSING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input text: {text_q1}\")\n",
    "print(\"\\nProcessing Steps:\")\n",
    "print(f\"  1. Tokenization: {len(tokens)} tokens\")\n",
    "print(f\"  2. Stopword removal: {len(filtered_tokens)} tokens remaining\")\n",
    "print(f\"  3. POS tagging & filtering: {len(filtered_pos)} nouns/verbs\")\n",
    "print(f\"  4. Lemmatization: {len(lemmatized_words)} final words\")\n",
    "print(f\"\\nFinal Output: {lemmatized_words}\")\n",
    "\n",
    "print(\"\\nStep-by-step Transformation:\")\n",
    "print(\"  Original → Tokenized → Stopwords Removed → POS Filtered → Lemmatized\")\n",
    "print(f\"  {len(text_q1):2d} chars   → {len(tokens):2d} tokens → {len(filtered_tokens):2d} tokens       → {len(filtered_pos):2d} tokens    → {len(lemmatized_words):2d} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Named Entity Recognition with Pronoun Detection\n",
    "\n",
    "**Requirements:**\n",
    "1. Perform Named Entity Recognition (NER)\n",
    "2. If text contains pronouns (\"he\", \"she\", \"they\"), print warning message\n",
    "\n",
    "**Input Text:**\n",
    "> \"Chris met Alex at Apple headquarters in California. He told him about the new iPhone launch.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text for Q2:\n",
      "\"Chris met Alex at Apple headquarters in California. He told him about the new iPhone launch.\"\n",
      "\n",
      "Text length: 92 characters\n"
     ]
    }
   ],
   "source": [
    "# Input text for Q2\n",
    "text_q2 = \"Chris met Alex at Apple headquarters in California. He told him about the new iPhone launch.\"\n",
    "\n",
    "print(\"Input Text for Q2:\")\n",
    "print(f'\"{text_q2}\"')\n",
    "print(f\"\\nText length: {len(text_q2)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pronoun Detection and Ambiguity Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Pronoun Detection\n",
      "==================================================\n",
      "Pronouns being checked: ['he', 'she', 'they', 'He', 'She', 'They']\n",
      "Tokens in text: ['Chris', 'met', 'Alex', 'at', 'Apple', 'headquarters', 'in', 'California', '.', 'He', 'told', 'him', 'about', 'the', 'new', 'iPhone', 'launch', '.']\n",
      "\n",
      "Found pronouns: ['He']\n",
      "\n",
      "[!]==================================================\n",
      "[!]  WARNING: Possible pronoun ambiguity detected!\n",
      "[!]==================================================\n",
      "\n",
      "Explanation: The text contains pronouns ['He'] which could refer to multiple entities.\n",
      "This creates ambiguity in understanding who is performing the actions.\n"
     ]
    }
   ],
   "source": [
    "# Define pronouns to check\n",
    "pronouns = ['he', 'she', 'they', 'He', 'She', 'They']\n",
    "\n",
    "# Tokenize the text for pronoun checking\n",
    "q2_tokens = word_tokenize(text_q2)\n",
    "found_pronouns = [word for word in q2_tokens if word in pronouns]\n",
    "\n",
    "print(\"Step 1: Pronoun Detection\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Pronouns being checked: {pronouns}\")\n",
    "print(f\"Tokens in text: {q2_tokens}\")\n",
    "print(f\"\\nFound pronouns: {found_pronouns}\")\n",
    "\n",
    "if found_pronouns:\n",
    "    print(\"\\n[!]\" + \"=\"*50)\n",
    "    print(\"[!]  WARNING: Possible pronoun ambiguity detected!\")\n",
    "    print(\"[!]\" + \"=\"*50)\n",
    "    print(f\"\\nExplanation: The text contains pronouns {found_pronouns} which could refer to multiple entities.\")\n",
    "    print(\"This creates ambiguity in understanding who is performing the actions.\")\n",
    "else:\n",
    "    print(\"\\n[+] No ambiguous pronouns detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Named Entity Recognition with spaCy\n",
    "\n",
    "Using spaCy's powerful NER capabilities to identify entities like persons, organizations, locations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Named Entity Recognition\n",
      "==================================================\n",
      "Named Entities Found:\n",
      "----------------------------------------\n",
      "  📍 'Chris          ' → PERSON     (Position: 0-5)\n",
      "  📍 'Alex           ' → PERSON     (Position: 10-14)\n",
      "  📍 'Apple          ' → ORG        (Position: 18-23)\n",
      "  📍 'California     ' → GPE        (Position: 40-50)\n",
      "  📍 'iPhone         ' → ORG        (Position: 78-84)\n",
      "\n",
      "Total entities found: 5\n",
      "\n",
      "Entity Label Explanations:\n",
      "  PERSON  : People, including fictional\n",
      "  ORG     : Companies, organizations\n",
      "  GPE     : Countries, cities, states\n",
      "  PRODUCT : Objects, vehicles, foods, etc.\n"
     ]
    }
   ],
   "source": [
    "# Perform NER using spaCy\n",
    "doc = nlp(text_q2)\n",
    "\n",
    "print(\"Step 2: Named Entity Recognition\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Named Entities Found:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "entities = []\n",
    "for ent in doc.ents:\n",
    "    entities.append((ent.text, ent.label_))\n",
    "    print(f\"  📍 '{ent.text:15}' → {ent.label_:10} (Position: {ent.start_char}-{ent.end_char})\")\n",
    "\n",
    "print(f\"\\nTotal entities found: {len(entities)}\")\n",
    "\n",
    "# Explain entity labels\n",
    "print(\"\\nEntity Label Explanations:\")\n",
    "print(\"  PERSON  : People, including fictional\")\n",
    "print(\"  ORG     : Companies, organizations\")\n",
    "print(\"  GPE     : Countries, cities, states\")\n",
    "print(\"  PRODUCT : Objects, vehicles, foods, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize NER Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Analysis Visualization\n",
      "==================================================\n",
      "Text with entities highlighted:\n",
      "----------------------------------------\n",
      "  Chris           → PERSON   entity\n",
      "  Alex            → PERSON   entity\n",
      "  Apple           → ORG      entity\n",
      "  California      → GPE      entity\n",
      "  iPhone          → ORG      entity\n",
      "\n",
      "Sentence Structure:\n",
      "  Chris[PERSON] met Alex[PERSON] at Apple[ORG] headquarters in California[GPE].\n",
      "  He[PRONOUN] told him[PRONOUN] about the new iPhone[PRODUCT] launch.\n",
      "\n",
      "Ambiguity Analysis:\n",
      "  The pronoun 'He' could refer to: Chris or Alex\n",
      "  The pronoun 'him' could refer to: Chris or Alex\n"
     ]
    }
   ],
   "source": [
    "print(\"NER Analysis Visualization\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Text with entities highlighted:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create a simple visualization\n",
    "colored_text = text_q2\n",
    "entity_colors = {\n",
    "    'PERSON': 'PERSON',\n",
    "    'ORG': 'ORG', \n",
    "    'GPE': 'GPE',\n",
    "    'PRODUCT': 'PRODUCT'\n",
    "}\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ in entity_colors:\n",
    "        marker = f\"[{ent.text}]({ent.label_})\"\n",
    "        print(f\"  {ent.text:15} → {ent.label_:8} entity\")\n",
    "\n",
    "print(\"\\nSentence Structure:\")\n",
    "print(\"  Chris[PERSON] met Alex[PERSON] at Apple[ORG] headquarters in California[GPE].\")\n",
    "print(\"  He[PRONOUN] told him[PRONOUN] about the new iPhone[PRODUCT] launch.\")\n",
    "\n",
    "print(\"\\nAmbiguity Analysis:\")\n",
    "print(\"  The pronoun 'He' could refer to: Chris or Alex\")\n",
    "print(\"  The pronoun 'him' could refer to: Chris or Alex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Complete Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2: COMPLETE NER ANALYSIS SUMMARY\n",
      "============================================================\n",
      "Input text: Chris met Alex at Apple headquarters in California. He told him about the new iPhone launch.\n",
      "\n",
      "Pronoun Analysis:\n",
      "  Pronouns found: ['He']\n",
      "  Ambiguity warning: YES\n",
      "\n",
      "Named Entity Recognition:\n",
      "  Total entities identified: 5\n",
      "    - Chris (PERSON)\n",
      "    - Alex (PERSON)\n",
      "    - Apple (ORG)\n",
      "    - California (GPE)\n",
      "    - iPhone (ORG)\n",
      "\n",
      "Key Insights:\n",
      "  1. Multiple PERSON entities (Chris, Alex) create pronoun ambiguity\n",
      "  2. The pronouns 'He' and 'him' lack clear antecedents\n",
      "  3. Context suggests technology/business setting (Apple, iPhone)\n"
     ]
    }
   ],
   "source": [
    "print(\"Q2: COMPLETE NER ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input text: {text_q2}\")\n",
    "print(f\"\\nPronoun Analysis:\")\n",
    "print(f\"  Pronouns found: {found_pronouns}\")\n",
    "print(f\"  Ambiguity warning: {'YES' if found_pronouns else 'NO'}\")\n",
    "print(f\"\\nNamed Entity Recognition:\")\n",
    "print(f\"  Total entities identified: {len(entities)}\")\n",
    "for entity, label in entities:\n",
    "    print(f\"    - {entity} ({label})\")\n",
    "    \n",
    "print(f\"\\nKey Insights:\")\n",
    "print(\"  1. Multiple PERSON entities (Chris, Alex) create pronoun ambiguity\")\n",
    "print(\"  2. The pronouns 'He' and 'him' lack clear antecedents\")\n",
    "print(\"  3. Context suggests technology/business setting (Apple, iPhone)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Both Q1 and Q2 have been successfully implemented:\n",
    "\n",
    "## Q1 Achievements:\n",
    "- Successful tokenization of input text\n",
    "- Effective stopword removal \n",
    "- Accurate POS tagging and noun/verb filtering\n",
    "- Proper lemmatization (context-aware, not just stemming)\n",
    "\n",
    "## Q2 Achievements:\n",
    "- Comprehensive pronoun detection with ambiguity warnings\n",
    "- Accurate Named Entity Recognition using spaCy\n",
    "- Clear identification of entities and their types\n",
    "- Detailed analysis of potential ambiguity issues\n",
    "\n",
    "The implementation demonstrates robust NLP processing capabilities using both NLTK and spaCy libraries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
